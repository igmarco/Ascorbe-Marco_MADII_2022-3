---
title: "Trabajo MADII - Análisis multivariante de un conjunto de datos"
author: "Pablo Ascorbe Fernández e Ignacio Marco Pérez"
date: "2023-01-30"
output: html_document
---

```{=html}
<style type="text/css">

h1.title {
  font-size: 38px;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-size: 22px;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-size: 20px;
  text-align: center;
}
</style>
```

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

MADII - Análisis multivariante de un conjunto de datosEl objetivo de este documento es explicar paso por paso el código introducido en los scripts (homónimos al nombre que daremos a cada sección) que se encuentran en la carpeta `/Scripts` del proyecto, correspondientes a la cada una de las fases de estudio del dataset "good_reads_final", disponible en el sitio web de [Kaggle](https://www.kaggle.com/code/jotamaggi/good-reads-author-gender-analysis).

El dataset en cuestión contiene información sobre un total de 22.891 libros de literatura extraídos de la página web [GoodReads](https://www.goodreads.com/), donde podemos encontrar los datos relativos a la identificación y detalles del autor y del libro, así como a las valoraciones de ambos. Este dataset nos ha resultado especialmente interesante, ya que, además de esta información, cada registro almacena el lugar de nacimiento y el género del autor, así como la fecha de publicación y los géneros principales del libro.

Una descripción detallada de cada variable podría ser la siguiente:

-   `author_average_rating`: Valoración media (0-5) en las obras publicadas del autor en la página.
-   `author_gender`: Género (`male` o `female`) del autor.
-   `author_genres`: Géneros principales de las obras publicadas del autor en la página.
-   `author_id`: Código identificador del autor en la página.
-   `author_name`: Nombre completo del autor.
-   `author_page_url`: Código URL de la página principal del autor.
-   `author_rating_count`: Número total de valoraciones de las obras publicadas del autor en la página.
-   `author_review_count`: Número de reseñas de las obras publicadas del autor en la página.
-   `birthplace`: Lugar de nacimiento (ciudad, estado o país) del autor.
-   `book_average_rating`: Valoración media (0-5) del libro en la página.
-   `book_fullurl`: Código URL de la página del libro.
-   `book_id`: Código identificador del libro en la página.
-   `book_title`: Título del libro.
-   `genre_1`: Género principal.
-   `genre_2`: Género secundario más relevante.
-   `num_ratings`: Número de valoraciones en la página.
-   `num_reviews`: Número de reseñas en la página.
-   `pages`: Número de páginas.
-   `publish_date`: Fecha de publicación
-   `score`: Información desconocida. Para el estudio, prescindiremos de esta variable.

De estas variables que hemos definido, realmente solo nos interesa trabajar con un subconjunto de ellas. Además, no todas están realmente estandarizadas (por ejemplo, las fechas de publicación no siguen un formato unificado). Por ello, uno de los primeros esfuerzos necesarios para llevar a cabo el estudio es el de preprocesar el dataset.

## 0. Preparación del proyecto

### 0.1. Creación de la estructura del proyecto

El primer paso para construir el proyecto es crear la estructura de este. Podemos utilizar el siguiente código (aunque la estructura ya está definida).

```{r}
if(!dir.exists("../Datos")){
  dir.create("../Datos")
}
if(!dir.exists("../Datos/Brutos")){
  dir.create("../Datos/Brutos")
}
if(!dir.exists("../Datos/Procesados")){
  dir.create("../Datos/Procesados")
}

if(!dir.exists("../Scripts")){
  dir.create("../Scripts")
}

if(!dir.exists("../Figuras")){
  dir.create("../Figuras")
}

if(!dir.exists("../Informes")){
  dir.create("../Informes")
}
```

### 0.2. Funciones auxiliares

Definamos las funciones auxiliares que emplearemos en fases posteriores del proyecto.

La función `asim` calcula el coeficiente de asimetría de un vector de valores (no nulos o nulos).

```{r}
asim <- function(x){
  x <- x[!is.na(x)]
  n <- length(x)
  return(mean((x - mean(x))^3)/(sd(x)^3))
}
```

La función `getYear` me permite obtener el año de un string que contenga el año como última subcadena tras un caracter `' '`.

```{r}
getYear <- function(dateString){
  return(unlist(strsplit(dateString, " "))[length(unlist(strsplit(dateString, " ")))] )
}
```

La función `mode` calcula la moda de un vector de valores (no nulos o nulos).

```{r}
mode <- function(x) {
  x <- x[!is.na(x)]
  return(names(which.max(table(x))))
}
```

La función `mcov` calcula la matriz de covarianzas de un dataframe `X` bajo la definición de covarianza estudiada en clase.

```{r}
mcov <- function(X){
  n <- dim(X)[1]
  return(cov(X)*(n - 1)/n)
}
```

### 0.3. Preprocesado de datos

Para llevar a cabo el preprocesado de los datos, extraeremos el DataFrame del archivo `good_reads_final.csv` que se encuentra en la carpeta `\Datos\Brutos`.

Lo primero que hacemos es instalar e importar los paquetes que utilizaremos en esta sección.

```{r, echo=FALSE}
if(!require(readr)){
  install.packages("readr", dependencies = TRUE, repos='http://cran.rstudio.com/')
  require(readr)
}
if(!require(psych)){
  install.packages("psych")
  require(psych)
}
if(!require(dplyr)){
  install.packages("dplyr")
  require(dplyr)
}
```

#### Importación y estudio inicial del DataFrame

Importamos el DataFrame.

```{r}
ruta_csv <- "../Datos/Brutos/good_reads_final.csv"
good_reads <- as.data.frame(read_csv(ruta_csv, col_names = TRUE))
```

Mostramos las columnas del DataFrame y vemos que coinciden con las definidas anteriormente.

```{r}
colnames(good_reads)
```

Para poder hacernos una idea inicial de cómo son los datos, mostramos los primeros 5 registros

```{r}
head(good_reads, 5)
```

Ahora que hemos visto más o menos la forma de los datos, hagamos un pequeño resumen de las variables.

```{r}
summary(good_reads)
```

Para ver un resumen más detallado utilizamos la función `describe`.

```{r}
describe(good_reads)
```

Las columnas que realmente nos interesan para el estudio son las siguientes:

-   `author_average_rating`
-   `author_gender`
-   `author_genres`
-   `author_rating_count`
-   `author_review_count`
-   `birthplace`
-   `book_average_rating`
-   `genre_1`
-   `genre_2`
-   `num_ratings`
-   `num_reviews`
-   `pages`
-   `publish_date`

De modo que eliminamos las variables irrelevantes.

```{r}
good_reads <- select(good_reads,-'author_id',-'author_name',-'author_page_url',
                     -'book_fullurl',-'book_id',-'book_title')

colnames(good_reads)
```

Para poder trabajar cómodamente con las columnas numéricas y categóricas, anotamos en dos vectores aquellas que corresponde con una categoría u otra.

```{r}
variables_categoricas <- c("author_gender",
                           "author_genres",
                           "birthplace",
                           "genre_1",
                           "genre_2")
variables_numericas <- c("author_average_rating",
                         "author_rating_count",
                         "author_review_count",
                         "book_average_rating",
                         "num_ratings",
                         "num_reviews",
                         "pages",
                         "publish_date")
```

#### Estudio de las variables categóricas del DataFrame

Para poder visualizar mejor los datos, mostramos los posibles valores que toma cada variable en el dataset.

```{r}
unique(good_reads$author_gender)
```

```{r}
unique(good_reads$author_genres)[0:10]
```

```{r}
unique(good_reads$birthplace)[0:10]
```

```{r}
unique(good_reads$genre_1)[0:10]
```

```{r}
unique(good_reads$genre_2)[0:10]
```

Vemos que las dos variables que necesitan preprocesado son `author_genres` y `birthplace`.

Para tratar `author_genres`, convertimos el string segmentado por el caracter ',' en un vector de componentes (géneros del autor).

```{r}
good_reads$author_genres <- strsplit(as.character(good_reads$author_genres), ",")
unique(good_reads$author_genres)[1:10]
good_reads[970,"author_genres"]
```

Ahora, para tratar `birthplace`, debemos eliminar los caracteres indeseables de los datos.

```{r}
good_reads$birthplace <- gsub("\n   ", "", good_reads$birthplace)
good_reads$birthplace <- gsub("\n  ", "", good_reads$birthplace)
good_reads$birthplace <- gsub("[\n\r]", "", good_reads$birthplace)
unique(good_reads$birthplace)[1:10]
```

Todavía vemos valores anómalos, así que los sustituimos por valores nulos.

```{r}
good_reads$birthplace[good_reads$birthplace == ""] <- NA
good_reads$birthplace[good_reads$birthplace == "Occupied"] <- NA
good_reads$birthplace[good_reads$birthplace == "Republic of"] <- NA
good_reads$birthplace[good_reads$birthplace == "Identity of author to remamystery for now"] <- NA
good_reads$birthplace[good_reads$birthplace == "I've continued \"worlding\" to many countries."] <- NA
```

Ahora, homogeneizamos el dataset quedándonos únicamente con el país (convirtiendo aquellos datos que contengan información sobre ciudades, regiones, estados, etc.).

```{r}
good_reads$birthplace[good_reads$birthplace == "Seoul"] <- "South Korea"

good_reads$birthplace[good_reads$birthplace == "British India" |
                        good_reads$birthplace == "ndia"] <- "India"

good_reads$birthplace[good_reads$birthplace == "Hong Kong"] <- "China"

good_reads$birthplace[good_reads$birthplace == "Cape Town"] <- "South Africa"

good_reads$birthplace[good_reads$birthplace == "Melbourne"] <- "Australia"

good_reads$birthplace[good_reads$birthplace == "Jerusalem"] <- "Israel"

good_reads$birthplace[good_reads$birthplace == "the Former Yugoslav Republic of"] <- "Yugoslav Republic"

good_reads$birthplace[good_reads$birthplace == "Holy See (vatican City State)"] <- "Vatican City State"

good_reads$birthplace[good_reads$birthplace == "Winnipeg" | 
                      good_reads$birthplace == "currently living Canada" | 
                        good_reads$birthplace == "MB  Canada" | 
                        good_reads$birthplace == "Vancouver" | 
                        good_reads$birthplace == "Canada "] <- "Canada"

good_reads$birthplace[good_reads$birthplace == "Birmingham" |
                        good_reads$birthplace == "London" |
                        good_reads$birthplace == "British Indian Ocean Territory" |
                        good_reads$birthplace == "Gibraltar" |
                        good_reads$birthplace == "Buckinghamshire" |
                        good_reads$birthplace == "North Wales" |
                        good_reads$birthplace == "Westminister CA" |
                        good_reads$birthplace == "Northern Ireland" |
                        good_reads$birthplace == "British" |
                        good_reads$birthplace == "Lancashire" |
                        good_reads$birthplace == "Scotland" |
                        good_reads$birthplace == "Wales"] <- "United Kingdom"

good_reads$birthplace[good_reads$birthplace == "Colorado" | 
                        good_reads$birthplace == "Chicago" | 
                        good_reads$birthplace == "Alabama" | 
                        good_reads$birthplace == "Southern California" | 
                        good_reads$birthplace == "New York" | 
                        good_reads$birthplace == "New Jersey" | 
                        good_reads$birthplace == "Missouri" | 
                        good_reads$birthplace == "Hawaii" | 
                        good_reads$birthplace == "Boston" | 
                        good_reads$birthplace == "Pennsylvania" | 
                        good_reads$birthplace == "Massachusetts" | 
                        good_reads$birthplace == "Kentucky" | 
                        good_reads$birthplace == "Michigan" | 
                        good_reads$birthplace == "Minnesota" | 
                        good_reads$birthplace == "Springfield" | 
                        good_reads$birthplace == "Minneapolis" | 
                        good_reads$birthplace == "Salt Lake City" | 
                        good_reads$birthplace == "Maryland" | 
                        good_reads$birthplace == "Jordan" | 
                        good_reads$birthplace == "New York City" | 
                        good_reads$birthplace == "Baltimore" | 
                        good_reads$birthplace == "Illinois" | 
                        good_reads$birthplace == "Ohio" | 
                        good_reads$birthplace == "New York City " | 
                        good_reads$birthplace == "Rocky Mount" | 
                        good_reads$birthplace == "Philadelphia" | 
                        good_reads$birthplace == "Indianapolis" | 
                        good_reads$birthplace == "Cleveland" | 
                        good_reads$birthplace == "Minot" | 
                        good_reads$birthplace == "New York " | 
                        good_reads$birthplace == "Ohio " | 
                        good_reads$birthplace == "Dallas" | 
                        good_reads$birthplace == "Indiana" | 
                        good_reads$birthplace == "North Carolina" | 
                        good_reads$birthplace == "Los Angeles" | 
                        good_reads$birthplace == "San Francisco" | 
                        good_reads$birthplace == "New Orleans" | 
                        good_reads$birthplace == "Wichita Falls" | 
                        good_reads$birthplace == "South Dakota" | 
                        good_reads$birthplace == "Brooklyn" | 
                        good_reads$birthplace == "California" | 
                        good_reads$birthplace == "a phoenix egg" | 
                        good_reads$birthplace == "Rhode Island" | 
                        good_reads$birthplace == "Ft. Lauderdale" | 
                        good_reads$birthplace == "http://nikkiloftin.com/" | 
                        good_reads$birthplace == "Virginia" | 
                        good_reads$birthplace == "Florida" | 
                        good_reads$birthplace == "Alaska" | 
                        good_reads$birthplace == "Gary IndiANa" | 
                        good_reads$birthplace == "Seattle" | 
                        good_reads$birthplace == "Delaware" | 
                        good_reads$birthplace == "Manhattan" | 
                        good_reads$birthplace == "KY " | 
                        good_reads$birthplace == "D.C." | 
                        good_reads$birthplace == "N.Y." | 
                        good_reads$birthplace == "WI" | 
                        good_reads$birthplace == "IL" | 
                        good_reads$birthplace == "MA" | 
                        good_reads$birthplace == "CA" | 
                        good_reads$birthplace == "CT" | 
                        good_reads$birthplace == "Somewhere snowy MN" | 
                        good_reads$birthplace == "AL" | 
                        good_reads$birthplace == "DC" | 
                        good_reads$birthplace == "NYC" | 
                        good_reads$birthplace == "IL " | 
                        good_reads$birthplace == "MN" | 
                        good_reads$birthplace == "VA" | 
                        good_reads$birthplace == "NY" | 
                        good_reads$birthplace == "AZ" | 
                        good_reads$birthplace == "KS" | 
                        good_reads$birthplace == "MI" | 
                        good_reads$birthplace == "OH" | 
                        good_reads$birthplace == "KY" | 
                        good_reads$birthplace == "TX" | 
                        good_reads$birthplace == "NH" | 
                        good_reads$birthplace == "PA"] <- "United States"

good_reads$birthplace[grep("France", good_reads$birthplace)] <- "France"
good_reads$birthplace[grep("Russia", good_reads$birthplace)] <- "Russia"
good_reads$birthplace[grep("Pakistan", good_reads$birthplace)] <- "Pakistan"

unique(good_reads$birthplace)
```

#### Estudio de las variables numéricas del DataFrame

```{r}
describe(good_reads[variables_numericas])
```

La única variable cuya información no parece estar bien procesada es `publish_date`.

```{r}
unique(good_reads$publish_date)[1:10]
```

Vemos que los datos no siguen una norma. Para ello, nos quedamos únicamente con los años en una variable `publish_year`, utilizando la función definida anteriormente.

```{r}
good_reads$publish_year <- sapply(good_reads$publish_date, getYear)

unique(good_reads$publish_year)
```

Sigue habiendo valores atípicos, así que los eliminamos.

```{r}
good_reads$publish_year[good_reads$publish_year == "by" |
                          good_reads$publish_year == "1" |
                          good_reads$publish_year == "0" |
                          substr(good_reads$publish_year,nchar(good_reads$publish_year)-1,nchar(good_reads$publish_year)+1) == "st" |
                          substr(good_reads$publish_year,nchar(good_reads$publish_year)-1,nchar(good_reads$publish_year)+1) == "nd" |
                          substr(good_reads$publish_year,nchar(good_reads$publish_year)-1,nchar(good_reads$publish_year)+1) == "rd" |
                          substr(good_reads$publish_year,nchar(good_reads$publish_year)-1,nchar(good_reads$publish_year)+1) == "th"] <- NA

unique(good_reads$publish_year)
```

Guardemos la información como enteros.

```{r}
good_reads$publish_year <- strtoi(good_reads$publish_year)
```

Por último, eliminemos la variable `publish_date`, ya que ya no la necesitaremos.

```{r}
good_reads <- select(good_reads,-'publish_date')
```

#### Exportación de los datos procesados

Como tenemos una variable que utiliza como datos vectores (`author_genres`), no podemos exportar los datos a un fichero CSV sin realizar alguna operación antes. Pero, como nuestro objetivo es guardar el DataFrame con los datos procesados, realizamos la copia en un fichero binario RDA que contenga directamente el objeto DataFrame.

```{r}
ruta_proc_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
save(good_reads, file = ruta_proc_rda)
```

Podemos comprobar que se ha guardado correctamente volviéndolo a cargar.

```{r}
ruta_read_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
load(file = ruta_read_rda)

head(good_reads)
```

## 1. Análisis previo

Vamos a estudiar el dataset de forma general. Para ello, veamos la información que nos dan los distintos resúmenes de datos para todo el DataFrame o para submuestras de este y representemos dicha información mediante los gráficos estudiados en la asignatura.

Primero, instalemos los paquetes necesarios para este apartado.

```{r}
if(!require(readr)){
  install.packages("readr", dependencies = TRUE, repos='http://cran.rstudio.com/')
  require(readr)
}
if(!require(psych)){
  install.packages("psych")
  require(psych)
}
if(!require(matrixStats)){
  install.packages("matrixStats")
  require(matrixStats)
}

if(!require(car)){
  install.packages("car")
  require(car)
}
if(!require(dplyr)){
  install.packages("dplyr")
  require(dplyr)
}
if(!require(ggplot2)){
  install.packages("ggplot2")
  require(ggplot2)
}
if(!require(GGally)){
  install.packages("GGally")
  require(GGally)
}
```

Llevamos a cabo la importación del DataFrame que generamos en la sección anterior.

```{r}
ruta_read_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
load(file = ruta_read_rda)
```

Ahora podemos trabajar con `good_reads` como antes.

```{r}
head(good_reads, 5)
```

#### Resúmenes

Veamos un resumen general de las variables.

```{r}
summary(subset(good_reads, select = -c(author_genres)))
```

Volvemos a declarar los vectores de variables categóricas y numéricas.

```{r}
variables_categoricas <- c("author_gender",
                           "author_genres",
                           "birthplace",
                           "genre_1",
                           "genre_2")
variables_numericas <- c("author_average_rating",
                         "author_rating_count",
                         "author_review_count",
                         "book_average_rating",
                         "num_ratings",
                         "num_reviews",
                         "pages",
                         "publish_year")
```

Pasamos a estudiar las variables numéricas. Obtengamos para cada una de ellas los tres estadísticos básicos: media, desviación típica según la definición de R y coeficiente de asimetría.

```{r}
colMeans(good_reads[variables_numericas], na.rm=TRUE)
```

```{r}
apply(good_reads[variables_numericas], 2, sd, na.rm=TRUE)
```

```{r}
apply(good_reads[variables_numericas], 2, asim)
```

Como resultados interesantes podemos ver, por ejemplo, que las valoraciones de libros (y por tanto de autores) tienen una media de un 3.95, con una desviación típica de 0.29.

También podemos ver que las variables como `author_rating_count`, `author_review_count`, `num_ratings` y `num_reviews` tienen una desviación típica muy alta, acompañadas de un elevado coeficiente de asimetría positivo, lo que indica que, aunque la media de estos valores sea relativamente baja, existe una gran cola de la distribución hacia la derecha.

Además, podemos ver que la mayoría de los valores se concentran respecto al origen y tienen un alto coeficiente de asimetría (concretamente todas aquellos cuyas variables sean distintas a las puntuaciones) hacia el lado en el que se alejan de él. Para la fecha de publicación, podemos considerar que el "origen" se encuentra en la actualidad, momento en el que más libros se publican y registran en la base de datos de Good Reads.

Pasemos a mostrar el resumen detallado de todas las variables numéricas.

```{r}
describe(good_reads[variables_numericas])
```

Respecto a las variables categóricas, veamos los datos que toma el dataset.

```{r}
unique(good_reads$author_gender)
```

```{r}
unique(good_reads$birthplace)
```

```{r}
unique(good_reads$genre_1)
```

```{r}
unique(good_reads$genre_2)
```

#### Subconjuntos

Para poder extraer los resultados correspondientes a libros de autores masculinos o femeninos, creamos dos subconjuntos de los datos, uno para cada género.

```{r}
good_reads_female <- subset(good_reads,
                      author_gender == 'female',
                      -author_gender)
head(good_reads_female)
```

```{r}
good_reads_male <- subset(good_reads,
                          author_gender == 'male',
                          -author_gender)
head(good_reads_male)
```

Comparemos los estadísticos básicos de ambos subconjuntos.

```{r}
colMeans(good_reads_female[variables_numericas], na.rm=TRUE)
colMeans(good_reads_male[variables_numericas], na.rm=TRUE)
```

```{r}
apply(good_reads_female[variables_numericas], 2, sd, na.rm=TRUE)
apply(good_reads_male[variables_numericas], 2, sd, na.rm=TRUE)
```

```{r}
apply(good_reads_female[variables_numericas], 2, asim)
apply(good_reads_male[variables_numericas], 2, asim)
```

Podemos observar cómo, en general, independientemente de que haya registrados más libros de hombres que de mujeres, estos son más populares y mejor valorados cuando son escritos por autores del género masculino. Esto puede deberse a múltiples causas, como las barreras sexistas en el mundo del arte o el hecho de que hasta hace poco el número de escritoras era muy reducido (tiene sentido que los libros registrados en Good Reads más antiguos sean precisamente obras clásicas populares). Esto último puede ser analizado al estudiar la correlación entre variables, como veremos más adelante.

En vez de extraer directamente los valores de estadísticos de ambos subconjuntos, podemos utilizar directamente la función `aggregate`, que nos permite dividir el total de los registros en base a variables o conjuntos de ellas.

Obtengamos la media de las valoraciones de los libros escritos por hombres y mujeres (obtendremos el mismo estadístico que antes) y la media de las valoraciones de los libros escritos por hombres y mujeres por cada género.

```{r}
aggregate(book_average_rating ~ author_gender,
          data = good_reads,
          FUN = mean, na.rm=TRUE)
```

```{r}
aggregate(book_average_rating ~ author_gender + genre_1,
          data = good_reads,
          FUN = mean, na.rm=TRUE)
```

Podemos ver, por ejemplo, que la evaluación media de los libros cuyo género es "Feminism" escritos por mujeres es mayor que la de libros escritos por hombres. Debemos tener en cuenta que con esto no estamos indicando cuántos libros corresponden a cada agrupación, por lo que es posible que los resultados para algunos géneros poco extendidos no sean representativos.

Veamos el número de libros que hay en nuestra base de datos escritos por hombres y mujeres en cada país.

```{r}
aggregate(book_average_rating ~ author_gender + birthplace,
          data = good_reads,
          FUN = length)
```

Podemos ver, como curiosidad, que los 40 libros escritos por autores españoles están escritos por hombres. Esto puede tener cierto sentido, ya que el momento de esplendor de la literatura española coincidió con una época en la que pocas mujeres eran autoras de libros de literatura. Si estudiamos los tres estadísticos básicos por lugar de nacimiento y nos fijamos en el de España vemos que la mayor parte de los libros que toma son de antes del 2000.

```{r}
aggregate(publish_year ~ birthplace,
          data = good_reads,
          FUN = function(x) return(c(mean(x, na.rm=TRUE), 
                                     sd(x, na.rm=TRUE), 
                                     asim(x))))
```

Si no se muestra el resultado en R Markdown (en el script de R puede verse el valor de la función `aggregate`), es posible ver el valor de España en el siguiente código:

```{r}
aggregate(publish_year ~ birthplace,
          data = good_reads,
          FUN = function(x) return(as.list(c(mean(x, na.rm=TRUE), 
                                     sd(x, na.rm=TRUE), 
                                     asim(x)))))[106,2]
```

#### Matriz de varianzas-covarianzas

Para poder obtener los resultados, hemos de eliminar los registros con valores nulos (aquellos que introdujimos en la sección de preprocesamiento) o imputarlos con algún estadístico. Para evitar perder información que podría sernos de utilidad, elegimos identificarlos e imputarlos.

```{r}
apply(good_reads, 2, function(x) return(sum(is.na(x))))
```

Vemos que solo se encuentran en las variables `birthplace`, `pages` y `publish_year`. Imputamos respectivamente los valores con la moda, la media y la mediana de dichas variables. Para `publish_year` escogemos imputar mediante la mediana en lugar de con la media (que es lo habitual) debido a la alta asimetría que hay en dichos datos.

```{r}
good_reads$birthplace[is.na(good_reads$birthplace)] <- mode(good_reads$birthplace)
good_reads$pages[is.na(good_reads$pages)] <- mean(good_reads$pages, na.rm=TRUE)
good_reads$publish_year[is.na(good_reads$publish_year)] <- median(good_reads$publish_year, na.rm=TRUE)
```

Ahora, calculamos sin problema la matriz de varianzas-covarianzas bajo las definiciones de R (cuasi-) y la estudiada en clase.

```{r}
cov(good_reads[variables_numericas])
```

```{r}
mcov(good_reads[variables_numericas])
```

Calculamos la matriz de correlaciones.

```{r}
cor(good_reads[variables_numericas])
```

#### Visualización de los datos

Para poder representar volúmenes de datos tan grandes (más de 20.000 registros), vamos a reducir el tamaño del dataset utilizando la función `sample` (más concretamente la función `sample_frac`) para un 1% de los datos originales. Para poder trabajar con las columnas numéricas, vamos a guardar dos samples: `sample_good_reads_catGender`, que contendrá la información numérica sobre la submuestra más la variable categórica `author_gender`, y `sample_good_reads`, que almacenará la misma información con la salvedad de que `author_gender` registrará un 0 para las mujeres y un 1 para los hombres.

```{r}
sample_good_reads <- sample_frac(good_reads, size = .01, replace = F)
sample_good_reads <- sample_good_reads[c("author_gender", variables_numericas)]
sample_good_reads_catGender <- sample_good_reads

sample_good_reads$author_gender <- ifelse(sample_good_reads$author_gender == "female", 0, 1)
```

##### Visualización general

Vamos a obtener la representación general de nuestros datos.

```{r, echo=FALSE}
scatterplotMatrix(sample_good_reads, pch = 19, smooth = FALSE, regLine = FALSE)
```

Aunque podemos entrever algunas relaciones básicas (como la alta correlación que existe entre las variables de puntuación del autor y sus obras o entre las variables de los números de reseñas y los números de valoraciones), es difícil extraer resultados con claridad. En la $Figura1$ se encuentra la imagen con mayor detalle.

Veamos la información por separado de las variables relativas al autor y de las variables relativas al libro. Aunque para hacer un estudio más realista sobre los autores deberíamos agrupar los registros por identificador de autor (variable que eliminamos en el preprocesamiento), dejaremos las filas tal y como están, otorgando por tanto un mayor peso lineal a los autores que más obras tengan registradas en la página.

```{r, echo=FALSE}
ggpairs(sample_good_reads_catGender, 
        columns=c("author_average_rating",
                  "author_rating_count",
                  "author_review_count"),
        mapping=aes(color=author_gender))
```

```{r, echo=FALSE}
ggpairs(sample_good_reads_catGender, 
        columns=c("book_average_rating",
                   "num_ratings",
                   "num_reviews",
                   "pages",
                   "publish_year"),
        mapping=aes(color=author_gender))
```

Es posible visualizar estas figuras con más detalle en la $Figura2.1$ y la $Figura2.2$.

##### Histogramas

Pasemos a representar un par de histogramas interesantes. Primero, veamos la distribución del número de páginas.

```{r, echo=FALSE}
ggplot(sample_good_reads_catGender, 
       aes(x = pages)) + 
  geom_histogram()
```

Vemos que la distribución que se sigue es relativamente normal, con la salvedad de la cola que se extiende hacia la derecha.

Veamos ahora los histogramas de evaluaciones (resultados y número) por géneros.

```{r, echo=FALSE}
ggplot(sample_good_reads_catGender, 
       aes(x = author_average_rating, 
           fill = author_gender)) + 
  geom_histogram()
```

```{r, echo=FALSE}
ggplot(sample_good_reads_catGender, 
       aes(x = author_rating_count, 
           fill = author_gender)) + 
  geom_histogram()
```

Realmente, es complicado extraer conclusiones directamente de estas gráficas, pero sí podemos ver, por ejemplo, cómo los libros de mayor popularidad y mejor valorados son mayoritariamente escritos por hombres. Es posible ver con mayor detalle estas gráficas en la $Figura3.1$, $Figura3.2$ y $Figura3.3$

##### Diagramas de cajas

Para poder visualizar cómodamente los datos asociados a los distintos géneros, filtramos primero aquellos géneros (o combinaciones de ellos) más comunes.

```{r}
good_reads_genres <- good_reads %>% group_by(genre_1,genre_2)
good_reads_genres_count <- good_reads_genres %>% summarise(copies = n())

mostCommonGenres <- head(good_reads_genres_count[order(good_reads_genres_count$copies, decreasing=TRUE),1:2],20)
```

Visualicemos las combinaciones de géneros con más registros en nuestro dataset.

```{r}
mostCommonGenres
```

Nos quedamos solo con los registros que tengan estas combinaciones.

```{r, include=FALSE}
good_reads_mostCommonGenres <- filter(good_reads_genres, 
                                      any(mostCommonGenres$genre_1 == genre_1 & 
                                            mostCommonGenres$genre_2 == genre_2))

good_reads_mostCommonGenres$genres <- paste(good_reads_mostCommonGenres$genre_1, 
                                            '-', 
                                            good_reads_mostCommonGenres$genre_2)
```

Realizamos el mismo proceso para los 5 lugares de nacimiento más comunes con otra copia del DataFrame distinta.

```{r}
good_reads_birthplace <- good_reads %>% group_by(birthplace)
good_reads_birthplace_count <- good_reads_birthplace %>% summarise(copies = n())

mostCommonBirthplaces <- head(good_reads_birthplace_count[order(good_reads_birthplace_count$copies, decreasing=TRUE),1],5)
```

```{r}
mostCommonBirthplaces
```

```{r, include=FALSE}
good_reads_mostCommonBirthplaces <- filter(good_reads_birthplace, 
                                      any(mostCommonBirthplaces$birthplace == birthplace))
```

Representamos los diagramas de cajas correspondientes a las siguientes relaciones:

-   Número de páginas por géneros.

```{r, echo=FALSE}
p0 <- ggplot(good_reads_mostCommonGenres, aes(genres, pages)) + 
  labs(x = 'Género', y = 'Páginas')
p0 + coord_flip() + geom_boxplot()
p0 + coord_flip() + geom_point()
```

-   Como apenas es posible ver nada debido a los outliers, filtramos los datos poniendo un número máximo de páginas para los registros.

```{r, echo=FALSE}
p1 <- ggplot(good_reads_mostCommonGenres[good_reads_mostCommonGenres$pages < 1500,], aes(genres, pages)) + 
  labs(x = 'Género', y = 'Páginas')
p1 + coord_flip() + geom_boxplot()
```

-   Número de valoraciones por géneros.

```{r, echo=FALSE}
p2 <- ggplot(good_reads_mostCommonGenres[good_reads_mostCommonGenres$num_ratings < 5e+5,], aes(genres, num_ratings)) + 
  labs(x = 'Género', y = 'Valoraciones')
p2 + coord_flip() + geom_boxplot()
```

-   Puntuaciones medias por géneros.

```{r, echo=FALSE}
p31 <- ggplot(good_reads_mostCommonGenres, aes(genres, book_average_rating)) + 
  labs(x = 'Género', y = 'Puntuación media')
p31 + coord_flip() + geom_boxplot()
```

-   Puntuaciones medias por lugar de nacimiento.

```{r, echo=FALSE}
p32 <- ggplot(good_reads_mostCommonBirthplaces, aes(birthplace, book_average_rating)) + 
  labs(x = 'Lugar de nacimiento', y = 'Puntuación media')
p32 + coord_flip() + geom_boxplot()
```

-   Fechas de publicación por géneros.

```{r, echo=FALSE}
p41 <- ggplot(good_reads_mostCommonGenres[good_reads_mostCommonGenres$publish_year > 1850,], aes(genres, publish_year)) + 
  labs(x = 'Género', y = 'Fecha de publicación')
p41 + coord_flip() + geom_boxplot()
```

-   Fechas de publicación por lugares de nacimiento.

```{r, echo=FALSE}
p42 <- ggplot(good_reads_mostCommonBirthplaces[good_reads_mostCommonBirthplaces$publish_year > 1850,], aes(birthplace, publish_year)) + 
  labs(x = 'Lugar de nacimiento', y = 'Fecha de publicación')
p42 + coord_flip() + geom_boxplot()
```

De estas gráficas (que se pueden encontrar con mayor detalle en la $Figura4.1$, $Figura4.2$, $Figura5$, $Figura6$, $Figura7.1$, $Figura8.1$ y $Figura8.2$) se puede extraer información interesante. Por ejemplo:

-   Los libros de Fantasía-Ficción parecen ser los más largos, mientras que los libros de Artes son los más cortos. Esto tiene sentido, ya que normalmente los libros de arte suelen tener forma de ensayos cortos mientras que los libros de fantasía suelen desarrollar grandes universos con largas descripciones.

-   Los libros Clásicos de Ficción suelen ser los que más valoraciones tienen, mientras que los de Romance los que menos. Esto también es normal. Los libros más antiguos tenderán a ser los más populares, mientras que los relatos románticos suelen tener un público objetivo más reducido.

-   Por algún motivo, los libros de género Juvenil suelen ser los peores valorados. Por otro lado, aquellos con contenido más "objetivo" (como los de Srte o los Históricos) suelen tener valoraciones más altas.

-   Los libros de autores japoneses tienen las valoraciones medias más altas, además de tener la menor desviación típica. Quizá las valoraciones de lectores japoneses sean menos variables, o tal vez se dediquen a géneros específicos cuyas puntuaciones medias sean más altas (como veíamos en el anterior punto).

-   Los géneros Clásicos de ficción son los que más libros tienen publicados en fechas más antiguas, mientras que otros más nuevos como New Adult, comienzan con sus primeros títulos ya en el último milenio.

-   La mayor parte de libros publicados en épocas más antiguas son Europeos, mientras que en la actualidad han sido superados por las nuevas potencias (especialmente angloparlantes), como EEUU, Japón, Canadá o Australia.

##### Gráficos de barras

Para poder representar en gráficos de barras la información relativa a las punutaciones de los distintos libros, vamos a dividirla en los tres terciles.

```{r}
good_reads$rating_range <- ifelse(good_reads$author_average_rating < as.numeric(quantile(good_reads$author_average_rating,0.333)), 
                                  'Bajo',
                                  ifelse(good_reads$author_average_rating < as.numeric(quantile(good_reads$author_average_rating,0.666)), 
                                         'Medio',
                                         'Alto'))

good_reads$rating_range <- factor(good_reads$rating_range, levels=c('Bajo','Medio','Alto'))
```

Representemos ahora la cantidad de obras publicadas por hombres y mujeres para cada tercil.

```{r, echo=FALSE}
ggplot(good_reads, aes(x = author_gender, fill = author_gender)) +
  geom_bar() +
  scale_fill_manual(values = c("pink1","steelblue1")) +
  facet_wrap(~rating_range) +
  labs(title = 'Distribuciones de valoración (0-5) de autores 
agrupadas por "hombre" y "mujer" en tres intervalos')
```

Podemos observar cómo las obras con puntuaciones más extremas suelen ser publicadas por hombres. Particularmente, hay una diferencia bastante grande en las obras del tercer tercil.

Pasemos a estudiar concretamente las obras más extremas; aquellas que se sitúen en el primer y en el último percentil.

```{r}
good_reads$rating_extreme_range <- ifelse(good_reads$author_average_rating < as.numeric(quantile(good_reads$author_average_rating,0.01)), 
                                  'Muy bajo',
                                  ifelse(good_reads$author_average_rating < as.numeric(quantile(good_reads$author_average_rating,0.99)), 
                                         'Medio',
                                         'Muy alto'))

good_reads$rating_extreme_range <- factor(good_reads$rating_extreme_range, levels=c('Muy bajo','Medio','Muy alto'))
```

```{r, echo=FALSE}
ggplot(good_reads[good_reads$rating_extreme_range != 'Medio',], aes(x = author_gender, fill = author_gender)) +
  geom_bar() +
  scale_fill_manual(values = c("pink1","steelblue1")) +
  facet_wrap(~rating_extreme_range) +
  labs(title = 'Distribuciones de valoración (0-5) de autores 
agrupadas por "hombre" y "mujer" en intervalos extremos')
```

Observamos cómo se cumple lo que comentábamos anteriormente. Los dos últimos gráficos pueden verse con más detalle en la $Figura9.1$ y en la $Figura9.2$.

## 2. Análisis de Componentes Principales (ACP)

En este apartado realizaremos el análisis de componentes principales, gracias a esta técnica podremos reducir la dimensión de los datos para poder visualizarlos y manejarlos mejor.

Primero, importaremos nuestros datos de nuevo, de modo que nos sirva este apartado de forma independiente al resto, como si partieramos de cero.

```{r}
if(!require(ggplot2)){
  install.packages("ggplot2")
  require(ggplot2)
}
if(!require(dplyr)){
  install.packages("dplyr")
  require(dplyr)
}
```

Importamos los datos.

```{r}
ruta_read_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
load(file = ruta_read_rda)
head(good_reads, 10)
```

Recordemos que teniamos valores nulos, visto en el apartado anterior así que en este caso los volvemos a imputar.

```{r}
good_reads$pages[is.na(good_reads$pages)] <- mean(good_reads$pages, na.rm=TRUE)
good_reads$publish_year[is.na(good_reads$publish_year)] <- median(good_reads$publish_year, na.rm=TRUE)
```

Tenemos variables categóricas y numéricas, para hacer este análisis vamos a quedarnos con las numéricas y usar el género principal para visualizar mejor los resultados, de entre todos los géneros nos quedaremos con los cinco más populares.

Nos quedamos con las variables deseadas y las guardamos en un dataframe.

```{r, warning=FALSE}
variables_numericas <- c("author_average_rating",
                         "author_rating_count",
                         "author_review_count",
                         "book_average_rating",
                         "num_ratings",
                         "num_reviews",
                         "pages",
                         "publish_year")

good_reads_genres <- good_reads %>% group_by(genre_1)
good_reads_genres_count <- good_reads_genres %>% summarise(copies = n())
mostCommonGenres <- head(good_reads_genres_count[order(good_reads_genres_count$copies, decreasing=TRUE),1:2],5)
good_reads_mostCommonGenres <- filter(good_reads_genres, 
                                      any(mostCommonGenres$genre_1 == genre_1))
good_reads_mostCommonGenres$genres <- paste(good_reads_mostCommonGenres$genre_1)
df_pca <- good_reads_mostCommonGenres[,c(variables_numericas, "genres")]
```

Separamos en X e y para el análisis.

```{r}
X <- as.matrix(df_pca[, -9])
```

Primero, vamos a hacer el análisis de componentes principales de manera "artesanal", así como vimos en la práctica de la asignatura por mayor completitud. Sabemos que no es necesario, pero sí interesante.

```{r}
p <- ncol(X)
S <- cov(X)
```

Calculamos la varianza total.

```{r}
var_tot <- sum(diag(S))
var_tot
```

Calculamos los valores y vectores propios.

```{r}
S_vvprop <- eigen(S)
sum(S_vvprop$values)
```

Vemos que corresponde con la suma de las varianzas de nuestras variables.

```{r}
100*cumsum(S_vvprop$values)/var_tot
```

Con tan solo las dos primeras componentes llegamos al 99% de varianza acumulada, aún así vamos a aplicar el método del codo para comprobarlo.

```{r, echo=FALSE}
plot(1:p,
     S_vvprop$values,
     type = "b",
     pch = 19,
     xlab = "#CP",
     ylab = "Varianza")
```

Es evidente que son dos componentes, con una serviría realmente, pero es preferible tener un par de componentes.

```{r}
k <- 2
X_star <- scale(X, scale = FALSE)
```

Calculamos las dos componentes principales y las guardamos en la matriz Y.

```{r}
Y <- X_star %*% S_vvprop$vectors[, 1:k]
summary(Y)
head(Y)
cov(Y)
S_vvprop$vectors[, 1:k]
```

Vemos los pesos que tienen cada una de las variables en las 2 componentes, comprobamos que las que más influyen son el número de reseñas y valoraciones tanto para los libros y los autores.

```{r}
cor(X, Y)
```

Vamos a mostrar los resultados.

```{r}
df_pca <- cbind(df_pca, Y)
colnames(df_pca)[10:11] <- paste("CP", 1:k, sep = "")
head(df_pca)
```

Y vamos visualizando las distintas gráficas.

```{r, echo=FALSE}
ggplot(df_pca, aes(x = CP1, y = CP2, color = genres, fill = genres)) + geom_point()
```

En la primera figura no llegamos a ver nada realmente útil, pero sí que encontramos algunos géneros distanciados del resto. Serán obras muy populares, con muchas reseñas y valoraciones. El resto está aglomerado en un cono y es dificil ver nada. Esto, tanto para la primera componente, como para la segunda.

```{r, echo=FALSE}
ggplot(df_pca, aes(x = CP1, y = genres)) + geom_point()
```

```{r, echo=FALSE}
ggplot(df_pca, aes(x = CP2, y = genres)) + geom_point()
```

En las otras dos figuras vemos como influyen las distintas componentes a los géneros. sí que se pueden llegar a apreciar qué géneros como fantasía, ficción y jóvenes adultos destacan por tener valores muy alejados del resto, es decir, muy populares. Otros géneros, según este dataset, no destacan tanto, como las novelas históricas o el romance contemporáneo.

Las figuras con mejor calidad pueden ser encontradas en la carpeta "./Figuras", éstas son: $Figura10.1$, $Figura10.2$, $Figura10.3$.

Pasamos a usar prcomp que es la librería pensada para hacer estos análisis.

```{r}
df_acp_cov <- prcomp(X)

df_acp_cov
```

Podemos ver que el resultados obtenido anteriormente es ligeramente distinto a este.

```{r, echo=FALSE}
plot(df_acp_cov, type = "l", pch = 19)
```

De nuevo, parece casi obligatorio quedarse con k = 2.

```{r}
cov(df_acp_cov$x[, 1:2])
df_acp_cov$rotation[, 1:2]
```

Vemos qué características ingluyen más en cada componente y que en general la hegemonía de las variables de valoraciones sobre las demás. Recuperamos el dataset original y le añadimos las dos componentes.

```{r}
df_pca <-
df_pca <- cbind(df_pca, df_acp_cov$x[, 1:2])
```

Hecho esto vamos a visualizar los resultados.

```{r, echo=FALSE}
ggplot(df_pca, aes(x = PC1, y = PC2, color = genres)) + geom_point()
```

```{r, echo=FALSE}
ggplot(df_pca, aes(x = PC1, y = genres)) + geom_point()
```

Vemos que los gráficos generados son prácticamente identicos a los obtenidos por el método artesanal. Sí que en el primero, se pueden apreciar grupos divididos, por valores distintos, en el eje de la primera componente, paralelos entre sí.

```{r, echo=FALSE}
biplot(df_acp_cov, xlabs = rep("·", nrow(df_pca)))
```

En esta gráfica, lo relevante son los ejes y las flechas, más que las observaciones, y es que solo dos variables tienen relevancia, "rating_count" para PC1 y "num_ratings" para PC2.

Mayor detalle de estas tres figuras en $Figura10.4$, $Figura10.5$ y $Figura10.6$.

Por terminar con este apartado, vamos a utilizar la matriz de correlaciones y no la de covarianzas para realizar el análisis.

```{r}
df_acp_corr <- prcomp(X, scale. = TRUE)
df_acp_corr
summary(df_acp_corr)
plot(df_acp_corr, type = "l", pch = 19)
```

Aquí si que es más complicado decidirse, y podría paracer que lo mejor es k=6, bueno... una menos porque el cambio se produce en el 5, pero por sencillez cogeremos k = 2.

```{r}
df_pca <- good_reads_mostCommonGenres[,c(variables_numericas, "genres")]
df_pca <- cbind(df_pca, df_acp_corr$x[, 1:2])
ggplot(df_pca, aes(x = PC1, y = PC2, color = genres, fill = genres)) +
  geom_point()
```

Realmente no se puede sacar demasiado en claso de esta gráfica, se nota que nuestras variables numéricas no pueden ser negativas y son cortadas en ese eje. No obstante, se pueden apreciar las tendencias de los géneros, pocos puntos veres o azules (historicos y romance en ese orden) se llegan a distanciar del conglomerado, mientras que hay aglomeraciones estiradas paralelas de géneros destacados, como fantasía o ficción.

Figura con mayor detalle en $Figura10.7$.

## 3. Análisis Factorial (AF)

Importación de paquetes.

```{r}
if(!require(psych)){
  install.packages("psych")
  require(psych)
}

if(!require(ggplot2)){
  install.packages("ggplot2")
  require(ggplot2)
}
```

Importamos los datos.

```{r}
ruta_read_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
load(file = ruta_read_rda)
```

Valores nulos (identificación e imputación).

```{r}
apply(good_reads, 2, function(x) return(sum(is.na(x))))
good_reads$pages[is.na(good_reads$pages)] <- mean(good_reads$pages, na.rm=TRUE)
good_reads$publish_year[is.na(good_reads$publish_year)] <- median(good_reads$publish_year, na.rm=TRUE)
```

Selección de variables numéricas.

```{r}
variables_numericas <- c("author_average_rating",
                         "author_rating_count",
                         "author_review_count",
                         "book_average_rating",
                         "num_ratings",
                         "num_reviews",
                         "pages",
                         "publish_year")

X <- good_reads[variables_numericas]
```

Análisis factorial "Artesanal" Conseguimos el número de variables y la matriz de correlaciones de nuestros datos.

```{r}
p <- ncol(X)
R <- cor(X)
R
```

Calculamos los valores y vectores propios.

```{r}
Rprop <- eigen(R)
Rprop
Rprop$vectors %*% diag(Rprop$values) %*% t(Rprop$vectors)
```

Porcentajes acumulados de varianza.

```{r}
cumvar <- 100*cumsum(Rprop$values)/p
```

Método del codo para decidir cuantos factores nos quedamos.

```{r, echo=FALSE}
plot(
  1:p,
  100 - cumvar,
  type = "b",
  pch = 19,
  xlab = "Variables",
  ylab = "Varianza acumulada"
)
```

Elegimos 2 factores y obtenemos la estimación de la matriz de cargas

```{r}
m <- 2
sum(Rprop$values[(m + 1):p]) # cota de la suma de cuadrados de la matriz residual
L <- (rep(1, p) %*% t(sqrt(Rprop$values[1:m]))) * Rprop$vectors[, 1:m]
rownames(L) <- colnames(X)
L
```

La primera variable habla de la popularidad y la segunda del nivel. Donde en la primera vemos que todas las variables relevantes son el número de reseñas y valoraciones, tanto como para el autor, como para la propia obra. Y la segunda, vemos que las dos variables más importantes hacen referencia a la puntuación media, tanto del autor, como de la obra.

Comunalidades:

```{r, echo=FALSE}
h2 <- rowSums(L^2)
h2
plot(
  1:p,
  h2,
  pch = 19,
  xlab = "Variables",
  ylab = "Comunalidad"
)
```

Vemos que las dos últimas variables "pages" y "publish_year" tienen una comunalidad prácticamente nula, y probablemente una unicidad muy elevada, eso lo veremos más adelante. El resto de variables tienen la comunalidad bastante alta, siendo las que más tienen "author_average_rating" y "book_average_rating".

Unicidades:

```{r}
psi <- 1 - h2
psi
```

```{r, echo=FALSE}
plot(
  1:p,
  psi,
  pch = 19,
  xlab = "Variables",
  ylab = "unicidad"
)
```

Como comentábamos antes, los valores se han intercambiado. En resumen, el modelo está bien ajustado, solo que dos variables se quedan bastante fuera. Recordemos que debemos tener un equilibrio entre tener poca unicidad y pocos factores. Lo suyo sería ver qué pasa si añadimos un tercer factor, pero eso es mejor que lo probemos cuando usemos la función "principal".

```{r, echo=FALSE}
ggplot(data.frame(cbind(1:p, h2)), aes(x = V1, y = h2, fill=1:p)) +
  geom_bar(stat='identity')
```

Vemos en este diagrama de barras, las seis variables con alta comunialidad y las dos variables con baja comunalidad.

Estimación de la matriz de correlaciones para posteriormente calcular los residuos.

```{r}
Rbarra <- L %*% t(L) + diag(psi)
Rbarra
```

Calculamos la matriz residual y a partir de ella su suma de cuadrados para tener una medida de calidad de ajuste.

```{r}
D <- R - Rbarra
sum(D^2)
```

Ahora vamos a comprobar que efectivamente se cumple que esta medida es menor que la suma de los 8 valores porpios de nuestra matriz.

```{r}
sum(D^2) <= sum(Rprop$values[3:p])
```

Vemos que se cumple y por ello, nuestra elección de dos factores es acertada, sí solo hubiesemos cogido uno puede que esto no se cumplíese.

Ahora vamos a mostrar gráficamente las distintas cargas en el espacio de los factores.

```{r, echo=FALSE}
L <- as.data.frame(L)
rownames(L) <- colnames(X)
colnames(L) <- c("F1", "F2")
ggplot(L, aes(x = F1, y = F2, label = rownames(L))) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_text()
```

Vemos que hay dos variables que se quedan en tierra de nadie, tanto publish_year, como pages. Y que las variables que comentabamos anteriormente son las que determinan cada uno de los factores, dándonos información sobre popularidad (F1) y sobre nivel o valoración (F2).

Podemos ver la figura con más detalle en $Figura11.1$.

Aplicamos la rotación para maximizar la varianza total.

```{r}
vmax_L <- varimax(as.matrix(L))
```

Y comprobamos los valores rotados.

```{r, echo=FALSE}
Lstar <- as.data.frame(vmax_L$loadings[, 1:m])
rownames(Lstar) <- colnames(X)
colnames(Lstar) <- c("F1", "F2")
ggplot(Lstar, aes(x = F1, y = F2, label = rownames(Lstar))) +
  geom_hline(yintercept = 0, lty = 2) +
  geom_vline(xintercept = 0, lty = 2) +
  geom_text()
```

Vemos que las cargas se han ajustado aún más a los ejes de cada factor y nuestras variables con alta unicidad poco siguen aportando.

Ahora, vamos a aplicar la función principal para hacer todo lo anterior de una manera mucho más cómoda y sencilla.

Antes cargamos los datos en X para mayor completitud.

```{r}
X <- good_reads[variables_numericas]
```

Comenzamos como antes con dos factores y sin rotación.

```{r}
mt_af <- principal(X, nfactors = 2, rotate = "none")
mt_af
mt_af$values
mt_af$loadings
mt_af$communality
```

Vamos a volver a mostrar las comunalidades y ver si las dos variables que tenían baja comunlaidad efectivamente mejorar si aumentamos el número de factores.

```{r, warning=FALSE}
ggplot(data.frame(cbind(1:p, mt_af$communality)), aes(x = X1, y = X2, fill=1:p)) +
  geom_bar(stat='identity')
```

Vemos que estas dos variables: año de publicación y número de páginas vuelven a tener poca comunalidad. De nuevo, vamos a aumentar el número de factores en pos de que mejoren.

```{r, echo=FALSE}
mt_af_3 <- principal(X, nfactors = 3, rotate = "none")
ggplot(data.frame(cbind(1:p, mt_af_3$communality)), aes(x = X1, y = X2, fill=1:p)) +
  geom_bar(stat='identity')
```

Como podemos comprobar, ahora sí que tenemos bajas unicidades, el número de páginas sigue teniendo una comunalidad baja, pero en general hemos mejorado estos números.

Podríamos seguir aumentando el número de factores y con ello mejorarían las comunalidades pero esto conllevaría un desequilibrío, ya que nuestro objetivo es minimizar el número de éstos.

```{r, echo=FALSE}
mt_af_4 <- principal(X, nfactors = 4, rotate = "none")
ggplot(data.frame(cbind(1:p, mt_af_4$communality)), aes(x = X1, y = X2, fill=1:p)) +
  geom_bar(stat='identity')
```

Como podemos comprobar, con 4 factores, ninguna variable tiene la comunalidad baja pero decidimos quedarnos con 3 para tratar de encontrar un equilibrio. Como hemos decidido quedarnos con tres factores, vamos a mostrar de nuevo para estos, los valores de cada variable:

```{r}
mt_af_3$loadings
```

Vemos que el primer factor hace referencia de nuevo a la popularidad, el segundo a el nivel o aprobación media y el tercero a libros con pocas páginas y recientes, que puede tener sentido para encontrar aquellos libros que son más o menos modernos y que no tienenuna barbaridad de páginas.

Ahora comprobemos la complejidad de Hoffman para cada variables, de modo que búscamos que cada una tenga un número cercano a cero.

Manera directa:

```{r}
mt_af_3$complexity
```

Manera artesanal:

```{r}
indice_Hoffman <- function(x) (sum(x^2)^2)/sum(x^4)
comp <- apply(mt_af_3$loadings, 1, indice_Hoffman)
comp
```

Calculamos la complejidad media.

```{r}
mean(comp)
```

Como vemos, tenemos una complejidad media de 1, lo cual es casi perfecto. Y en general, vemos que casi todas las variables tienen una complejidad cercana a 1, las más altas son: pages y author_average_rating.

Vemos como aportan cada una de las variables a cada factor con la siguiente función.

```{r}
fa.diagram(mt_af_3)
```

Figura con más detalle en $Figura11.2$.

Y ya por último, vamos a mostrar las comunalidades pero aplicando la rotación varimax.

```{r}
mt_af_3R <- principal(X, nfactors = 3)
mt_af_3R
```

```{r, echo=FALSE}
ggplot(data.frame(cbind(1:p, mt_af_3R$communality)), aes(x = X1, y = X2, fill=1:p)) +
  geom_bar(stat='identity')
```

Vemos que prácticamente no varia al aplicar esta rotación. Mostramos el digrama de mayor carga para cada variable.

```{r}
fa.diagram(mt_af_3R)
```

Como último apartado, vamos a comprobar los residuos de un análisis con una sola componente, para ver si es suficiente o no.

```{r}
L_1 <- (rep(1, p) %*% t(sqrt(Rprop$values[1:1]))) * Rprop$vectors[, 1:1]
h2_1 <- rowSums(L_1^2)
psi_1 <- 1 - h2_1
D_1 <- R - (L_1 %*%t(L_1) + diag(psi_1))
sum(D_1^2)
```

Vemos el error para dos factores.

```{r}
sum(D^2)
```

Como podemos comprobar, efectivamente no sería un buen ajuste, ya que la hemos cogido un solo factor y empeora mucho el error, con residuos muy altos.

## 4. Análisis de Conglomerados (AC)

En este apartado realizaremos un análisis de conglomerados y para esto aplicaremos el método de las

K-medias.

Importamos los paquetes.

```{r}
if(!require(ggplot2)){
  install.packages("ggplot2")
  require(ggplot2)
}
```

Importación de los datos.

```{r}
ruta_read_rda <- "../Datos/Procesados/good_reads_final_preprocesado.rda"
load(file = ruta_read_rda)
```

Valores nulos (identificación e imputación).

```{r}
apply(good_reads, 2, function(x) return(sum(is.na(x))))
good_reads$pages[is.na(good_reads$pages)] <- mean(good_reads$pages, na.rm=TRUE)
good_reads$publish_year[is.na(good_reads$publish_year)] <- median(good_reads$publish_year, na.rm=TRUE)
```

Selección de variables numéricas.

```{r}
variables_numericas <- c("author_average_rating",
                         "author_rating_count",
                         "author_review_count",
                         "book_average_rating",
                         "num_ratings",
                         "num_reviews",
                         "pages",
                         "publish_year")
```

Para poder mostrar los resultados, evidentemente, necesitamos dos dimensiones; para conseguirlas vamos a recuperar los resultados obtenidos del apartado 2 análisis de componentes principales, y así poder proyectar nuestros datos a dos dimensiones.

```{r}
X <- good_reads[variables_numericas]
df_pca <- prcomp(X, scale. = TRUE)
X_pca <- data.frame(df_pca$x[, 1:2])
```

Primero, al igual que vimos en la práctica informática vamos a hacer el método iterativo para comprobar como se van recalculando los centroides y con ello reagrupando los puntos.

Mostramos los puntos.

```{r, echo=FALSE}
plot(X_pca, pch = 19, col = 2)
```

Seleccionamos el número de grupos, lo haremos con k=2.

```{r}
K <- 2
```

Vamos a fijar una semilla.

```{r}
set.seed(123)
```

Calculamos los centroides inicales.

```{r}
centroides <- X_pca[sample(dim(X_pca)[1], K), ]
rownames(centroides) <- 1:K
centroides
```

```{r, echo=FALSE}
plot(X_pca, pch = 19, col = 2)
points(centroides, pch = 4, cex = 4, lwd = 4, col = 3)
```

Vemos que los dos centroides han caido entre la "masa" donde se aglomeran casi todos los puntos.

-   Primera iteración:

```{r}
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
head(X_pca)
```

Asignación de clúster; aquel cuyo centroide esté más cerca.

```{r}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
head(X_pca)
```

Actualización de centroides:

```{r}
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
centroides
```

Mostramos los resultados:

```{r, echo=FALSE}
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

Vemos los dos grupos que se han generado, el verde y el rosa. Ahora vamos a ir iterando hasta que se estabilicen los clústeres.

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
head(X_pca)
```

Comprobamos la varianza total de nuestros clústeres

```{r}
sum(X_pca$dist_centroid^2)
```

Vemos que nuestra varianza es alta, vamos a ir iterando y viendo como se reduce.

-   Iteración 2:

Antes de proceder a ella vamos a fijar un epsilon, cuando la resta entre la vaianza total de una iteración no supere a este epsilon en la siguiente pararemos.

```{r}
epsilon = 10
var_ant <- sum(X_pca$dist_centroid^2)

for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
head(X_pca)
```

```{r}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
head(X_pca)
```

```{r}
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
centroides
```

```{r, echo=FALSE}
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
head(X_pca)
```

Comprobamos la varianza total de nuestros clústeres:

```{r}
sum(X_pca$dist_centroid^2)
```

Comprobamos nuestro criterio de parada:

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

Vemos que no se cumple, seguimos. Aunque a partir de ahora solo mostraremos el gráfico, la varianza total y si se cumple o no la condición.

-    Iteración 3:

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

Sigue sin cumplirse, así que iteraremos hasta que lo haga.

-    Iteración 4:

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-    Iteración 5:

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-   Iteración 6:

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-    Iteración 7:

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-    Iteración 8

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-    Iteración 9

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-   Iteración 10

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

-    Iteración 11

```{r, echo=FALSE}
var_ant <- sum(X_pca$dist_centroid^2)
for(i in 1:K){
  txt <- paste("X_pca$distancia",
               i,
               "<- sqrt((X_pca$PC1 - centroides$PC1[",
               i,
               "])^2 + (X_pca$PC2 - centroides$PC2[",
               i,
               "])^2)",
               sep = "")
  eval(parse(text = txt))
}
X_pca$cluster <- apply(X_pca[, 3:(2 + K)], 1, which.min)
centroides <- aggregate(cbind(PC1, PC2) ~ cluster,
                        data = X_pca,
                        mean)
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

```{r}
X_pca$dist_centroid <- apply(X_pca[, 3:(2 + K)], 1, min)
sum(X_pca$dist_centroid^2)
```

```{r}
var_ant - sum(X_pca$dist_centroid^2) < epsilon
```

Como podemos comprobar, nuestro criterio de parada se ha cumplido, por lo tanto vamos a parar.

Los grupos generado finales son:

```{r, echo=FALSE}
plot(X_pca[, 1:2], pch = 19, col = X_pca$cluster + 1)
points(centroides[, 2:3], pch = 4, cex = 4, lwd = 4,
       col = centroides$cluster + 1)
```

Ahora vamos a utilizar la función kmeans y comparar resultados.

```{r}
datos_km <- kmeans(X_pca[, 1:2], 2, nstart = 1e3)
```

Calculamos la inercia del modelo

```{r}
datos_km$tot.withinss
```

Vemos que es menor que la que hemos conseguido nosotros de manera artesanal.

Mostramos los resultados:

```{r, echo=FALSE}
plot(X_pca[, 1:2], pch = 19, col = datos_km$cluster + 1)
```

Vemos que los grupos se han generado dividido por los valores de PC1, es decir, los libros con más valoraciones o más populares de los que no lo son. Mientras que en nuestro otro caso se dividian por los valores de PC2, dividiendo grupos de libros con valoraciones más altas, frente a los que tienen valoraciones más bajas.

## 5. Conclusiones

Trabajo terminado, como último apartado vamos, de una manera muy breve, a concluir los resultados conseguidos. Primero, no solo hemos conseguido familiarizarnos mejor con el entorno de RStudio, crenando proyectos y usando diferentes librerías, sino que hemos podido mejorar y desarrollar nuestras habilidades manejando R.

Segundo, trabajando con unos datos exógenos que no habíamos trabajado previamente, hemos aprendido a manejarnos con datos desconocidos y con un tema que nos parece interesante, como es el mundo de la literatura.

Por último, poder aplicar todos los conceptos vistos en la asignatura, hemos repasado su contenido teórico y aplicado éstos en nuestros datos, de manera que hemos logrado conclusiónes interesantes.

Gracias por la oportunidad realizando este trabajo, ha sido un placer.
